# -*- coding: utf-8 -*-
"""sr1part2LITR.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1hkDokuN81_SHMthc3KJnAtnb2-qyhcmK
"""

import requests
import pandas as pd
import string
from bs4 import BeautifulSoup
from google.colab import drive
import time

drive.mount('/drive')

base_link = "https://pubmed.ncbi.nlm.nih.gov"
article_df = pd.read_csv('/drive/My Drive/Scrappers/LITRScrap/sr1part2/review_443052_included_csv_20241123082518.csv')

articles = article_df["DOI"].tolist()

#articles = ['10.1002/da.23076']

spread = [['TY','TI','AU','JO','AB','OriginArticle','type']]
#Title Author Journal Abstract
failed = [['Title','link','title of reference/cited/similar article']]
#spread = [['origin_article','type','title','author','journal','PMID']]
#spread = [['DB','ID','TI','AU','TY','PY','JO','VL','IS','SP','EP','KW','AB','UR','DO','N1']]

for current_article_index in articles:
  if (type(current_article_index) == float):
    failed.append([current_article_index,"Not a valid DOI",""])
    continue
  print(["Currently working on: " + current_article_index])
  origin_article = current_article_index #current article title

  current_PM_search = base_link +'/?term='+ origin_article
  webpage = requests.get(current_PM_search)
  webpage_html = BeautifulSoup(webpage.content,'html5lib')
  article = webpage_html.find('div',attrs={'class':'article-page'})

  if (article != None):
    main = article.find('main',attrs={'class':'article-details'})

    #GETPMID
    header = main.find('header',attrs={'class':'heading'})
    My_PMID = header.find('strong',attrs={'class':'current-id'}).text

###########Get references##############
    referen_search = base_link + '/'+My_PMID+'/references/'
    refern_webpage = requests.get(referen_search)
    refern_html = BeautifulSoup(refern_webpage.content,'html5lib')
    references = refern_html.find('ol',attrs={'id':'full-references-list-1'})

    if (references != None):
      subcontent_li = references.find_all('ol',attrs={'class':'references-and-notes-list'})

      for i in subcontent_li:
        i = i.find('li',attrs={'class':'skip-numbering'})
        citation = i.get_text().strip().split('\n')[0]

        pmd_a_obj = i.find_all('a')
        heading_title = ""
        for obj in pmd_a_obj:
          if ("PubMed" in obj.text):
            pmd_ref_link = obj["href"]
        #Citation String Processing


            #abstract extraction

            abstract_request = base_link + pmd_ref_link
            sub_article = requests.get(abstract_request)
            sub_webpage_html = BeautifulSoup(sub_article.content,'html5lib')
            sub_article = sub_webpage_html.find('main',attrs={'class':'article-details'})

            if (sub_article != None):
              heading_title = sub_article.find('h1',attrs={'class':'heading-title'}).text
              abs = sub_article.find('div',attrs={'class':'abstract-content selected'})
              string = ""
              if (abs != None):
                paragraphs = abs.find_all('p')
                for par in paragraphs:
                  string  = string + par.text.strip()
              else:
                string = "no abstract available"
            else:
              failed.append([origin_article,"Pubmed Link No Longer Available",citation])
          #else:
            #spread.append([origin_article,"Reference outside of pubmed",citation])
        if (heading_title != ""):
          spread.append(["Article",heading_title.strip(),citation,"",string,origin_article,"Reference"])
        else:
          failed.append([origin_article,"No Title Found",citation])

    else:
      failed.append([origin_article,"No references"])

######CitedBy#########
    citeby_subsection = main.find('div',attrs={'class':'citedby-articles'})
    if(citeby_subsection!=None):
      citeby_subsection = citeby_subsection.find_all('li',attrs={'class':'full-docsum'})
      for j in citeby_subsection:

        title = "BROKEN"
        short_authors = "BROKEN"
        short_journal = "BROKEN"


        a_block = j.find('a')
        link = a_block['href']
        title = a_block.text.strip()
        abstract_request = base_link + link
        sub_article = requests.get(abstract_request)
        sub_webpage_html = BeautifulSoup(sub_article.content,'html5lib')
        sub_article = sub_webpage_html.find('main',attrs={'class':'article-details'})
        abs = sub_article.find('div',attrs={'class':'abstract-content selected'})
        string = ""
        if (abs != None):
          paragraphs = abs.find_all('p')
          for par in paragraphs:
            string  = string + par.text.strip()
        else:
          string = "no abstract available"


        #Main Article
        cite_subcontent= j.find('div',attrs={'class':'docsum-citation full-citation'})
        short_authors = cite_subcontent.find('span',attrs={'class':'docsum-authors short-authors'}).text
        short_journal= cite_subcontent.find('span',attrs={'class':'docsum-journal-citation short-journal-citation'}).text
        art_type="cited by"
        spread.append(["Article",title,short_authors,short_journal,string,origin_article,'Cited By'])

#####Similar####Articles######
    simart_subsection = main.find('div',attrs={'class':'similar-articles'})
    if (simart_subsection!=None):
      simart_subsection = simart_subsection.find_all('li',attrs={'class':'full-docsum'})
      for k in simart_subsection:

        title = "BROKEN"
        short_authors = "BROKEN"
        short_journal = "BROKEN"

        a_block = k.find('a')
        link = a_block['href']
        title = a_block.text.strip()
        abstract_request = base_link + link
        sub_article = requests.get(abstract_request)
        sub_webpage_html = BeautifulSoup(sub_article.content,'html5lib')
        sub_article = sub_webpage_html.find('main',attrs={'class':'article-details'})
        abs = sub_article.find('div',attrs={'class':'abstract-content selected'})
        string = ""
        if (abs != None):
          paragraphs = abs.find_all('p')
          for par in paragraphs:
            string  = string + par.text.strip()
        else:
          string = "no abstract available"
        #Main Article
        sim_subcontent= k.find('div',attrs={'class':'docsum-citation full-citation'})
        short_authors = sim_subcontent.find('span',attrs={'class':'docsum-authors short-authors'}).text
        short_journal= sim_subcontent.find('span',attrs={'class':'docsum-journal-citation short-journal-citation'}).text
        art_type = 'similar article'
        spread.append(["Article",title,short_authors,short_journal,string,origin_article,'Similar Article'])
  else:
    failed.append([origin_article,current_PM_search])

df = pd.DataFrame(spread)
faileddf = pd.DataFrame(failed)
df.to_csv('/drive/My Drive/Scrappers/LITRScrap/sr1part2/litreview.csv',index=False,header=None)
faileddf.to_csv('/drive/My Drive/Scrappers/LITRScrap/sr1part2/faileditems.csv',index=False,header=None)